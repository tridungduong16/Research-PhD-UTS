{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Run all the cells sequentially. \n",
    "2. The experiments below describe how we can adapt C-LRG in the paper https://arxiv.org/pdf/2010.15234.pdf for non-linear models. We run comparsions with F-IRM http://proceedings.mlr.press/v119/ahuja20a/ahuja20a.pdf\n",
    "3. We have comments in front of each command to guide one through the details of the implementation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Internal libraries summary\n",
    "1. data_constructor.py: in this file we define two classes <br>\n",
    "    assemble_data_mnist( ): for creating colored environments for MNIST digits <br>\n",
    "    assemble_data_mnist_fashion( ): for creating colored environments for MNIST fashion <br>\n",
    "\n",
    "\n",
    "2. IRM_methods.py: from this file we will use two classes <br>\n",
    "    a) fixed_irm_game_model for model F-IRM from http://proceedings.mlr.press/v119/ahuja20a/ahuja20a.pdf  <br>\n",
    "    b) fixed_irm_game_model_cons for adaptation of C-LRG for non-linear settings from https://arxiv.org/pdf/2010.15234.pdf <br>\n",
    "\n",
    "    \n",
    "    Each class is initialized using hyperparameters for the corresponding model.\n",
    "    Each class has a fit method, which takes as input the data from the different environments and trains the models. Finally, each class has an evaluation method, which takes the test data from test environment as input and outputs the accuracy on the test data and also on the train data that was used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_constructor.py explained\n",
    "\n",
    "The datasets used in Ahuja et.al. and Arjovsky et.al. essentially modified standard datasets such as MNIST digits, fashion MNIST to create multiple environments with different degrees of spurious correlations and the labels. Here we describe the classes that allow to create these datasets.\n",
    "\n",
    "    1. data_constructor.py consists of two classes: assemble_data_mnist() and assemble_data_mnist_fashion() \n",
    "        a) assemble_data_mnist()/assemble_data_mnist_fashion() has following functions \n",
    "            i) create_training_data(n_e, p_color_list, p_label_list):\n",
    "                n_e: number of environments, p_color_list: list of probabilities of switching the final label to obtain the color index, p_label_list: list of probabilities of switching pre-label\n",
    "            ii) create_testing_data(p_color_test, p_label_test, n_e): \n",
    "                n_e: number of environments, p_color_test: probability of switching the final label to obtain the color index in test environment, p_label_test: probability of switching pre-label in test environment\n",
    "        b)  assemble_data_mnist()/assemble_data_mnist_fashion() following attributes:\n",
    "            i) data_tuple_list: list of length n_e, each element of the list is a tuple with three elements (data, label, environment index)\n",
    "            ii) data_tuple_test: tuple with three elements (data_test, label_test, test environment index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRM_methods.py explained\n",
    "\n",
    "    1. fixed_irm_game_model class. Implements F-IRM game from http://proceedings.mlr.press/v119/ahuja20a/ahuja20a.pdf.\n",
    "    \n",
    "        A) Initialization:\n",
    "        fixed_irm_game_model(model_list, learning_rate, num_epochs, batch_size, termination_acc, warm_start)\n",
    "           i) model_list: list of models for each environment; use keras to construct the architectures\n",
    "           ii) learning_rate: learning rate for Adam optimizer for training the models for each environment\n",
    "           iii) batch_size: size of the batch used for each gradient update\n",
    "            iv) num_epochs: number of epochs is number of training steps = number of training samples//batch size (each epoch is one full pass of the training data)\n",
    "            v) termination_acc: once the model accuracy falls below this threshold we terminate training\n",
    "           vi) warm_start: minimum number of steps before we terminate due to accuracy falling below threshold\n",
    "\n",
    "        B) Methods:\n",
    "            i) fit(data_tuple_list): takes data_tuple_list and trains the models\n",
    "                   data_tuple_list- list of length n_e, each element of the list is a tuple with three elements (data, label, environment index)     \n",
    "            ii) evaluate(data_tuple_test): tuple with three elements (data_test, label_test, test environment index)\n",
    "\n",
    "        C) Attributes:\n",
    "            i) model_list: list of models for each environment\n",
    "            ii) train_acc: training accuracy (use after running evaluate method)\n",
    "            iii) test_acc: testing accuracy  (use after running evaluate method) \n",
    "\n",
    "    2. fixed_irm_game_model class_cons: Implements C-LRG https://arxiv.org/pdf/2010.15234.pdf  adapted for non-linear settings. This class has same structure as fixed_irm_game_model class\n",
    "    \n",
    "         \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# import cProfile\n",
    "import copy as cp\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_construct import * ## contains functions for constructing data \n",
    "from IRM_methods import *    ## contains IRM games methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits:  2 environments\n",
    "\n",
    "Below we illustrate how to use our IRM methods. \n",
    "We first setup the data in the cell below.  We set p_color_list = [0.2, 0.1] (from experiments in Arjovsky et.al.); note that there is marginal difference between the probabilities of switching the labels in the two environments. This marginal difference is useful for IRM methods to learn invariant predictors across environments that exploit the shape of digits and not the color. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare F-IRM and C-LRG adapted for standard colored MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize F-IRM model (we pass the hyper-parameters that we chose above)\n",
    "\n",
    "# Create data for each environment\n",
    "\n",
    "av_jm  = 0\n",
    "av_jm1 = 0\n",
    "\n",
    "for i in range(10):\n",
    "    print (\"trial\"+ str(i))\n",
    "    n_e = 2  # number of environments\n",
    "\n",
    "    p_color_list = [0.2, 0.1] # list of probabilities of switching the final label to obtain the color index\n",
    "    p_label_list = [0.25]*n_e # list of probabilities of switching pre-label\n",
    "    D = assemble_data_mnist() # initialize mnist digits data object\n",
    "\n",
    "    D.create_training_data(n_e, p_color_list, p_label_list) # creates the training environments\n",
    "\n",
    "    p_label_test = 0.25 # probability of switching pre-label in test environment\n",
    "    p_color_test = 0.9  # probability of switching the final label to obtain the color index in test environment\n",
    "\n",
    "    D.create_testing_data(p_color_test, p_label_test, n_e)  # sets up the testing environment\n",
    "    (num_examples_environment,length, width, height) = D.data_tuple_list[0][0].shape # attributes of the data\n",
    "    num_classes = len(np.unique(D.data_tuple_list[0][1])) # number of classes in the data\n",
    "\n",
    "    model_list = [] \n",
    "    for e in range(n_e):\n",
    "        model_list.append(keras.Sequential([\n",
    "                keras.layers.Flatten(input_shape=(length, width,height)),\n",
    "                keras.layers.Dense(390, activation = 'elu'),\n",
    "                 keras.layers.Dropout(0.75),\n",
    "                keras.layers.Dense(390, activation='elu'),\n",
    "                 keras.layers.Dropout(0.75),\n",
    "                keras.layers.Dense(num_classes)\n",
    "        ]))\n",
    "\n",
    "    num_epochs       = 25\n",
    "    batch_size       = 256\n",
    "    termination_acc  = 0.53\n",
    "    warm_start       = 100\n",
    "    learning_rate    = 5e-4\n",
    "    \n",
    "    \n",
    "    F_game = fixed_irm_game_model_cons(model_list, learning_rate, num_epochs, batch_size, termination_acc, warm_start) \n",
    "\n",
    "    # fit function runs the training on the data that we created\n",
    "    F_game.fit(D.data_tuple_list)\n",
    "\n",
    "    # evaluate function runs and evaluates train and test accuracy of the final model\n",
    "    F_game.evaluate(D.data_tuple_test) \n",
    "\n",
    "    # print train and test accuracy\n",
    "#     print (\"Training accuracy \" + str(F_game.train_acc)) \n",
    "    print (\"Testing accuracy \" + str(F_game.test_acc))\n",
    "    av_jm = av_jm+ F_game.test_acc\n",
    "    \n",
    "    model_list = [] \n",
    "    for e in range(n_e):\n",
    "        model_list.append(keras.Sequential([\n",
    "                keras.layers.Flatten(input_shape=(length, width,height)),\n",
    "                keras.layers.Dense(390, activation = 'elu'),\n",
    "                 keras.layers.Dropout(0.75),\n",
    "                keras.layers.Dense(390, activation='elu'),\n",
    "                 keras.layers.Dropout(0.75),\n",
    "                keras.layers.Dense(num_classes)\n",
    "        ]))\n",
    "        \n",
    "    F_game1 = fixed_irm_game_model(model_list, learning_rate, num_epochs, batch_size, termination_acc, warm_start) \n",
    "\n",
    "    # fit function runs the training on the data that we created\n",
    "    F_game1.fit(D.data_tuple_list)\n",
    "\n",
    "    # evaluate function runs and evaluates train and test accuracy of the final model\n",
    "    F_game1.evaluate(D.data_tuple_test) \n",
    "\n",
    "    # print train and test accuracy\n",
    "#     print (\"Training accuracy \" + str(F_game1.train_acc)) \n",
    "    print (\"Testing accuracy \" + str(F_game1.test_acc))\n",
    "    av_jm1 = av_jm1+ F_game1.test_acc\n",
    "\n",
    "print (\"F-IRM average accuracy:\" +  str(av_jm1/10) )# F_IRM\n",
    "print (\"C-LRG adapted for non-linear models average accuracy:\" + str(av_jm/10))  # C-LRG adapted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
