{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "GMM lecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o3CCzV_NZ2s",
        "colab_type": "text"
      },
      "source": [
        "# What is a Gaussian Mixture Model?\n",
        "\n",
        "A Gaussian mixture model (GMM) is a latent variable model of continuous data.  It assumes that each data point comes from one of several different Gaussian distributions.  The modeler assumes she knows the total number of Gaussians in the mixture.\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "![GMM](https://imgur.com/bRU3R6m.png) \n",
        "\n",
        "The figure on the left is a directed acyclic graph (DAG).  The figure on the right is the same model represented using [plate notation](https://en.wikipedia.org/wiki/Plate_notation).  Plate notation takes a set of nodes in the DAG that repeat and collapses them over one dimension into a single node.  The \"plates\" represent a single dimension.  \n",
        "\n",
        "There are two plates in our GMM.  One plate has size N=3 for the number of data points and one of size K = 2 for the number of components.  Next to each plate is a sampling statement that shows how to sample the vector of variables for that plate from conditional probability distributions.\n",
        "\n",
        "Plate notation is ideal for a specific model class like a GMM because the number of nodes in the DAG can vary from problem to problem, while the plate notation stays the same.\n",
        "\n",
        "## A GMM as a Causal Model\n",
        "\n",
        "You have probabbly never heard of a GMM being described as a causal model.  Indeed in most cases it would likely perform poorly as a causal model.  Later, we'll discuss how well this would fair as a causal model.\n",
        "\n",
        "However, for now, let's just realize the fact that we have a probabilistic generative model on a directed acyclic graph, so we can assume this is a causal model just by assuming the DAG represents causality.\n",
        "\n",
        "In this figure $X_1$, $X_2$ and $X_3$ are observed continuous random variables.  The fact that they are observed is indicated by grey.\n",
        "\n",
        "$Z_1$, $Z_2$, and $Z_3$ are latent (unobserved) discrete random variables.  The fact that they are latent is indicated by the white color of the node.\n",
        "\n",
        "Each observed node $X_i$ is sampled from either a Normal distribution with mean $\\mu_1$ or a Normal distribution with mean $\\mu_2$.\n",
        "\n",
        "### So what is the causal generative story?\n",
        "The _causal generative story_ is simply this; $Z_i$ causes $X_i$.  $Z_i$ is a switch mechanism that causes $X_i$ to have a value of either $\\mu_1$ plus noise or $\\mu_2$ plus noise or $\\mu_3$ plus noise.\n",
        "\n",
        "## Greeks vs. Romans\n",
        "\n",
        "We see two kinds of variable names in this representation.  Those named with letters from the Greek letters, and those named with letters from the Roman alphabet.\n",
        "\n",
        "So what should we think about these Greek letters?  They don't show up in the causal generative story.  Why are they in the graph?\n",
        "\n",
        "Here is how to understand the differences between the Greeks and the Romans.\n",
        "\n",
        "1. The Roman letters X and Z are the causally-related components of our data generating process.  \n",
        "2. The Greek letters $\\alpha$, $\\theta$ $\\sigma$, $\\sigma_0$ are parameters or weights.  These are merely parameters of the **causal Markov kernels**.\n",
        "\n",
        "A **causal Markov kernel** is just another name for the probability distribution of a variable conditional on its parents in the causal DAG.  The actual causal mechanism between the parents and the child determines (the word in the literature is \"entails\") this probability distribution.  If the causal model is correct, the causal Markov kernels should be invariant across data and domain.\n",
        "\n",
        "The parameters of the causal Markov kernel are explicitly in the graph because **we are thinking like Bayesians**.  In a previous lecture, we said that we should use probability to represent any uncertainty there is in elements of our \"data creation myth.\"  Generally, in probabilistic graphical models, random variables get their own nodes.  So a Bayesian using graphical modeling will represent parameters as random variables and thus nodes.  Explicitly modeling parameters in the graph structure allows them to use techniques from [Bayesian hierarchical modeling](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling) to model uncertainty in these parameters.\n",
        "\n",
        "However, from our causal perspective, explicit representations of these parameters distract us from the causal relationships we are assuming in our model.  We can get a view of those relationships by ignoring the Greek letters.\n",
        "\n",
        "![simpler viz](https://i.imgur.com/zdRzSa5.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N-4lJUHNZ2x",
        "colab_type": "text"
      },
      "source": [
        "### Simple example\n",
        "\n",
        "The following shows how to implement our GMM in Pyro.\n",
        "\n",
        "Fist lets import a bunch of things, not all of which will be needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqnKa7krf-SM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To install Pyro\n",
        "#!pip3 install torch torchvision\n",
        "#!pip3 install pyro-ppl \n",
        "\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from torch.distributions import constraints\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro import poutine\n",
        "from pyro.infer.autoguide import AutoDelta\n",
        "from pyro.optim import Adam\n",
        "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete\n",
        "\n",
        "smoke_test = ('CI' in os.environ)\n",
        "assert pyro.__version__.startswith('1.2.0')\n",
        "pyro.enable_validation(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rHqDNLwMgsO",
        "colab_type": "text"
      },
      "source": [
        "Next, let's specify the model.  The`config_enumerate` decorator is used in inference.  We don't need to worry about it for our learning purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_482mpuP9x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = 2\n",
        "\n",
        "@config_enumerate\n",
        "def model(N):\n",
        "    # Global variables.\n",
        "    α = 0.5\n",
        "    θ = pyro.sample('θ', dist.Dirichlet(α * torch.ones(K)))\n",
        "    σ = 1.0\n",
        "    σ_O = 10.\n",
        "    with pyro.plate('components', K):\n",
        "        μ = pyro.sample('μ', dist.Normal(0., σ_O))\n",
        "\n",
        "    with pyro.plate('data', N):\n",
        "        # Local variables.\n",
        "        Z = pyro.sample('Z', dist.Categorical(θ))\n",
        "        X = pyro.sample('X', dist.Normal(μ[Z], σ))\n",
        "    return {'X': X, 'Z': Z}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HVpjtqQK2Sw",
        "colab_type": "text"
      },
      "source": [
        "Notice how Pyro has a `pyro.plate` context manager that captures the \"plate\" abstraction in plate notation.  Also notice how the tensor representation provided by a deep generative modeling framework makes it convenient to capture variables within plates as vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFiAYBn9NbBE",
        "colab_type": "text"
      },
      "source": [
        "Now let's generate from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSICDB2gNeox",
        "colab_type": "code",
        "outputId": "c6afd9aa-21f6-41c8-b25c-3173defd2504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X': tensor([16.7073, 14.2783, 15.1040, 16.0092]), 'Z': tensor([0, 0, 0, 0])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0o4uLSMLgMz",
        "colab_type": "text"
      },
      "source": [
        "# Interventions\n",
        "\n",
        "Since this is a causal model, we can apply interventions.\n",
        "\n",
        "Pyro has a `pyro.do` function that will take in a model, and return a modified model that reflects the intervention.  It does this by replacing whatever sampling statement was used to generate the intervention target in the model with a statment that fixes that value to the intervention value.\n",
        "\n",
        "In the following code, I set 10 values of Z to [0, 1, 1, 0, 1, 1, 1, 1, 1, 1].  Then I generate from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZP_iy96IqPE",
        "colab_type": "code",
        "outputId": "63e11590-acbe-48df-a19b-311cc98c31ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "intervention = torch.tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
        "intervention_model = pyro.do(model, data={'Z': intervention})\n",
        "intervention_model(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X': tensor([13.9519,  6.3355,  5.5210, 14.3321,  5.0247,  8.2380,  5.2797,  5.2364,\n",
              "          6.1813,  4.7738]), 'Z': tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVZ-Y6g1N_D_",
        "colab_type": "text"
      },
      "source": [
        "Note the Z values are exactly what the intervention set them to.  The X values are forward generated from the Z values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy2q1JAu7zAj",
        "colab_type": "text"
      },
      "source": [
        "# Training the Greeks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJtSsFXtOPRb",
        "colab_type": "text"
      },
      "source": [
        "In latent variable modeling, the modeler generally doesn't know the values of the Greek variables.  In our case, we used probability distributions to capture that uncertainty.\n",
        "\n",
        "In practice, modelers try to infer their values from training data (i.e., values of Z's and X's).  In other words, we treat the Greeks as weights in a training step.\n",
        "\n",
        "There are several ways to learn these parameters from data.  Getting maximum likelihood estimates using expectation maximization is a common way.  Here, since we are thinking as Bayesians, we use Bayesian inference.\n",
        "\n",
        "A Bayesian inference algorithm will treat the probability distributions we gave to the unknown Greek letters as a prior distribution.  Given data, an inference algorithm will update these distributions.\n",
        "\n",
        "The following uses an [approximate Bayesian algorithm](https://en.wikipedia.org/wiki/Approximate_Bayesian_computation) called [stochastic variational inference](http://pyro.ai/examples/svi_part_i.html) (SVI).  SVI makes good use of the gradient-descent based optimization infrastructure of a deep learning framework like Pyro.  The following inference implementation will find [MAP estimates](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) of the Greek letters -- these are Bayesian analogs to maximum likelihood estimates.\n",
        "\n",
        "Do not be intimidated by the following code.  This is not unlike most deep learning code you see with deep learning libraries.  `TraceEnum_ELBO` and `SVI` are abstractions for stochastic variational inference.  I encourage you to learn more about Bayesian inference algorithms.  After all, knowledge of these algorithms tends to correlate with salary.  However, in these AltDeep causal modeling courses we only need a high-level understanding of inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg390HbW8P2g",
        "colab_type": "code",
        "outputId": "37239aed-2bbd-4a0e-d47d-4029f5878936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "data = torch.tensor([0., 1., 10., 11., 12.])\n",
        "N = len(data)\n",
        "K = 2  # Fixed number of components.\n",
        "evidence_model = pyro.condition(model, data={'X': data})\n",
        "\n",
        "optim = pyro.optim.Adam({'lr': 0.1, 'betas': [0.8, 0.99]})\n",
        "elbo = TraceEnum_ELBO(max_plate_nesting=1)\n",
        "\n",
        "def init_loc_fn(site):\n",
        "    if site[\"name\"] == \"θ\":\n",
        "        # Initialize weights to uniform.\n",
        "        return torch.ones(K) / K\n",
        "    if site[\"name\"] == \"μ\":\n",
        "        return data[torch.multinomial(torch.ones(N) / N, K)]\n",
        "    raise ValueError(site[\"name\"])\n",
        "\n",
        "def initialize(seed):\n",
        "    global global_guide, svi\n",
        "    pyro.set_rng_seed(seed)\n",
        "    pyro.clear_param_store()\n",
        "    global_guide = AutoDelta(poutine.block(evidence_model, expose=['θ', 'μ']),\n",
        "                             init_loc_fn=init_loc_fn)\n",
        "    svi = SVI(evidence_model, global_guide, optim, loss=elbo)\n",
        "    return svi.loss(evidence_model, global_guide, N)\n",
        "\n",
        "# Choose the best among 100 random initializations.\n",
        "loss, seed = min((initialize(seed), seed) for seed in range(100))\n",
        "initialize(seed)\n",
        "print('seed = {}, initial_loss = {}'.format(seed, loss))\n",
        "\n",
        "# Register hooks to monitor gradient norms.\n",
        "gradient_norms = defaultdict(list)\n",
        "for name, value in pyro.get_param_store().named_parameters():\n",
        "    value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item()))\n",
        "\n",
        "losses = []\n",
        "for i in range(200 if not smoke_test else 2):\n",
        "    loss = svi.step(N)\n",
        "    losses.append(loss)\n",
        "    print('.' if i % 100 else '\\n', end='')\n",
        "\n",
        "\n",
        "map_estimates = global_guide(N)\n",
        "θ = map_estimates['θ']\n",
        "μ = map_estimates['μ']\n",
        "print('/n')\n",
        "print('θ = {}'.format(θ.data.numpy()))\n",
        "print('μ = {}'.format(μ.data.numpy()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed = 19, initial_loss = 17.06005859375\n",
            "\n",
            "...................................................................................................\n",
            ".................................................................................................../n\n",
            "θ = [0.62500006 0.375     ]\n",
            "μ = [10.96146     0.49751246]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UYD_5ibRtAX",
        "colab_type": "text"
      },
      "source": [
        "Now that we have estimates for the value of our Greeks, we can replace their distributions in the model with these estimates values.  An even more ideal approach would be to sample them from distributions in `pyro.distributions` that were close in shape to the posteriors of these Greeks.\n",
        "\n",
        "## A word of caution on inference\n",
        "\n",
        "There is much to say about Bayesian inference.  This is not a course on inference so I don't say much and leave it to you to experiment with various inference abstractions in Pyro.\n",
        "\n",
        "However, there are some points worth mentioning when it comes to inferring the values of \"Greeks\" in causal models.  Firstly, getting these Greek letters right is of supreme importance in the common causal inference task of *inferring causal effects*, meaning quantifying the degree to which a cause influences an effect.\n",
        "\n",
        "The above inference algorithm assumes latent Z's, which is the usual case for GMM's.  Even if our causal model were a good one, trying to train model parameters when causes like Z are latent can lead to problems when trying to estimate these causal effects accurately.  We address this in the \"Identification and Estimation\" part of the causal modeling curriculum.\n",
        "\n",
        "Also, as a general rule, if you want an accurate estimation of the Greek variables, you should avoid approximate Bayesian algorithms in favor of exact ones (like MCMC approaches).  Approximate algorithms often ignore important nonlinearities in the causal mechanisms in exchange for speed and scalability.\n",
        "\n",
        "That said, if all we care about is getting reasonably good predictions of interventions, we might be okay if we had a good causal model.  Further, we could start with a basic GMM, then apply the **iterative refutation algorithm** (see lecture notes in Model-based Inference om Machine Learning)  to iterate on this model.  Each iteration we could retrain the model using new data from actual intervention experiments from the previous intervention, gradually overcoming estimation problems."
      ]
    }
  ]
}