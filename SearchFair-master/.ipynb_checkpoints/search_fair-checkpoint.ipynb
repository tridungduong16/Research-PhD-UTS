{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "__all__ = ['SearchFair']\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator\n",
    "import sklearn.metrics.pairwise as kernels\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import random\n",
    "\n",
    "\n",
    "class SearchFair(BaseEstimator):\n",
    "    \"\"\"SearchFair\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fairness_notions: string\n",
    "        The name of the fairness notion that the classifier should respect. 'DDP' or 'DEO' can be used.\n",
    "    fairness_regularizer: string\n",
    "        The name of the fairness relaxation that is used as a regularizer. It can be 'linear', or 'wu'. For 'wu', the 'wu_bound' can be chosen.\n",
    "    wu_bound: string\n",
    "        The name of the function that is used in the bounds of Wu et al. It can be 'hinge', 'logistic', 'squared', 'exponential'\n",
    "    reg_beta: float\n",
    "        Regularization parameter Beta for the l2 regularization.\n",
    "    kernel: string\n",
    "        The kind of kernel that is used. It can be 'linear', 'rbf' or 'poly'. For 'rbf' and 'poly', the parameter gamma can be used.\n",
    "    gamma: float\n",
    "        For kernel='rbf', gamma is the kernel width, for kernel='poly', gamma is the degree.\n",
    "    loss_name: string\n",
    "        The name of the loss used. Possible values: 'hinge', 'logistic', 'squared', 'exponential'\n",
    "    lambda_max: float\n",
    "        The value of lambda_max for the start of the binary search.\n",
    "    max_iter: int\n",
    "        The number of iterations of the solver chosen.\n",
    "    reason_points: float\n",
    "        The ratio of points used as reasonable points for the similarity-based approach of SearchFair.\n",
    "    stop_criterion: float\n",
    "        If SearchFair finds a classifier that is at least as fair as 'stop_criterion', than it stops the search.\n",
    "    max_search_iter: int\n",
    "        The number of iterations for the binary search.\n",
    "    solver: string\n",
    "        The solver that is used by cvxpy. It can be 'SCS' or 'ECOS'.\n",
    "    verbose: boolean\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_: numpy array\n",
    "        An array containing the trained weights for each reasonable point.\n",
    "    reason_pts_index: numpy array\n",
    "        An array containing the indices of the reasonable points in the training data.\n",
    "\n",
    "    Notes\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fairness_notion='DDP', fairness_regularizer='wu', wu_bound='hinge', reg_beta=0.001, kernel='linear', gamma=None, loss_name='hinge', lambda_max=1, max_iter=3000, reason_points=0.5, stop_criterion=0.01, max_search_iter=10, solver='SCS', verbose=False):\n",
    "\n",
    "        self.reg_beta = reg_beta\n",
    "        self.fairness_notion = fairness_notion\n",
    "        self.max_iter = max_iter\n",
    "        self.max_search_iter = max_search_iter\n",
    "        self.solver = solver\n",
    "        self.verbose = verbose\n",
    "        self.stop_criterion = stop_criterion\n",
    "        self.reason_points = reason_points\n",
    "        self.lambda_max = lambda_max\n",
    "        self.wu_bound = wu_bound\n",
    "        self.fairness_regularizer = fairness_regularizer\n",
    "        self.wu_bound = wu_bound\n",
    "        self.gamma = gamma\n",
    "        self.loss_name = loss_name\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, x_train, y_train, s_train=None):\n",
    "        \"\"\"Fits SearchFair on the given training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train: numpy array\n",
    "            The features of the training data with shape=(number_points,number_features).\n",
    "        y_train: numpy array\n",
    "            The class labels of the training data with shape=(number_points,).\n",
    "        s_train: numpy array\n",
    "            The binary sensitive attributes of the training data with shape=(number_points,).\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self: object\n",
    "        \"\"\"\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.s_train = s_train\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Preprocessing...\")\n",
    "        self._preprocess()\n",
    "\n",
    "        lbda_min, lbda_max = 0, self.lambda_max\n",
    "\n",
    "        def learn(reg, bound='upper'):\n",
    "            # If bound is None, we have decided which one to use, and we are in the middle of the binary search\n",
    "\n",
    "            self.fairness_lambda = reg\n",
    "            if bound is not None:\n",
    "                self._construct_problem(bound=bound)\n",
    "            self._optimize()\n",
    "            DDP, DEO = self.compute_fairness_measures(self.predict(x_train), y_train, s_train)\n",
    "            if self.fairness_notion == 'DDP':\n",
    "                fair_value = DDP\n",
    "            else:\n",
    "                fair_value = DEO\n",
    "            if self.verbose: print(\"Obtained:\",self.fairness_notion, \"= %0.4f with lambda = %0.4f\" % (fair_value, reg))\n",
    "            return fair_value, self.coef_.copy()\n",
    "\n",
    "        criterion = False\n",
    "\n",
    "        bound = 'upper' # even though an upper bound is specified, since lambda_min is 0, it falls away\n",
    "        if self.verbose: print(\"Testing lambda_min: %0.2f\" % lbda_min)\n",
    "        min_fair_measure, min_alpha = learn(lbda_min, bound=bound)\n",
    "        if np.sign(min_fair_measure) < 0: bound = 'lower'\n",
    "        if self.verbose: print(\"Testing lambda_max: %0.2f\" % lbda_max)\n",
    "        max_fair_measure, max_alpha = learn(lbda_max, bound)\n",
    "\n",
    "        if np.abs(min_fair_measure) < np.abs(max_fair_measure):\n",
    "            best_lbda, best_fair_measure = lbda_min, min_fair_measure\n",
    "            best_alpha = min_alpha\n",
    "        else:\n",
    "            best_lbda, best_fair_measure = lbda_max, max_fair_measure\n",
    "            best_alpha = max_alpha\n",
    "        if  np.abs(best_fair_measure) < self.stop_criterion:\n",
    "            print(\"Classifier is fair enough with lambda = {:.4f}\".format(best_lbda))\n",
    "        elif np.sign(min_fair_measure) == np.sign(max_fair_measure):\n",
    "            print('Fairness value has the same sign for lambda_min and lambda_max.')\n",
    "            print('Either try a different fairness regularizer or change the values of lambda_min and lambda_max') # Possibly, there could be a few more tries by reducing lambda.\n",
    "        else:\n",
    "            search_iter = 0\n",
    "            if self.verbose: print(\"Starting Binary Search...\")\n",
    "            while not criterion and search_iter < self.max_search_iter:\n",
    "                lbda_new = (lbda_min + lbda_max) / 2\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(10*'-'+\"Iteration #%0.0f\" % search_iter + 10*'-')\n",
    "                    print(\"Testing new Lambda: %0.4f\" % lbda_new)\n",
    "\n",
    "                new_rd, new_alpha = learn(lbda_new, None)\n",
    "                if np.abs(new_rd) < np.abs(best_fair_measure):\n",
    "                    best_fair_measure = new_rd\n",
    "                    best_lbda = lbda_new\n",
    "                    best_alpha = new_alpha.copy()\n",
    "\n",
    "                if np.sign(new_rd) == np.sign(min_fair_measure):\n",
    "                    min_fair_measure = new_rd\n",
    "                    lbda_min = lbda_new\n",
    "                else:\n",
    "                    max_fair_measure = new_rd\n",
    "                    lbda_max = lbda_new\n",
    "                if np.abs(new_rd) < self.stop_criterion:\n",
    "                    criterion = True\n",
    "\n",
    "                search_iter += 1\n",
    "            if search_iter==self.max_search_iter and self.verbose:\n",
    "                print(\"Hit maximum iterations of Binary Search.\")\n",
    "            elif self.verbose:\n",
    "                print(\"Sufficient fairness obtained before maximum iterations were reached.\")\n",
    "\n",
    "        if self.verbose: print(10*'-'+\"Found Lambda %0.4f with fairness %0.4f\" % (best_lbda, best_fair_measure)+10*'-')\n",
    "        self.coef_ = best_alpha.copy()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"Predict the label of test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_test: numpy array\n",
    "            The features of the test data with shape=(number_points,number_features).\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y_hat: numpy array\n",
    "            The predicted class labels with shape=(number_points,).\n",
    "        \"\"\"\n",
    "        kernel_matr = self.kernel_function(x_test, self.x_train[self.reason_pts_index])\n",
    "        y_hat = np.dot(self.coef_, np.transpose(kernel_matr))\n",
    "        return np.sign(y_hat)\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"Setting the attributes loss_func, kernel_function, and weight_vector,\n",
    "        which depends on the fairness notion, and is used in fairness related objects.\n",
    "        \"\"\"\n",
    "        self.coef_ = None\n",
    "        self.fairness_lambda = 0\n",
    "        if self.loss_name == 'logistic':\n",
    "            self.loss_func = lambda z: cp.logistic(-z)\n",
    "        elif self.loss_name == 'hinge':\n",
    "            self.loss_func = lambda z: cp.pos(1.0 - z)\n",
    "        elif self.loss_name == 'squared':\n",
    "            self.loss_func = lambda z: cp.square(-z)\n",
    "        elif self.loss_name == 'exponential':\n",
    "            self.loss_func = lambda z: cp.exp(-z)\n",
    "        else:\n",
    "            print('Using default loss: hinge loss.')\n",
    "            self.loss_func = lambda z: cp.pos(1.0 - z)\n",
    "\n",
    "        if self.kernel == 'rbf':\n",
    "            self.kernel_function = lambda X, Y: kernels.rbf_kernel(X, Y, self.gamma)\n",
    "        elif self.kernel == 'poly':\n",
    "            self.kernel_function = lambda X, Y: kernels.polynomial_kernel(X, Y, degree=self.gamma)\n",
    "        elif self.kernel == 'linear':\n",
    "            self.kernel_function = lambda X, Y: kernels.linear_kernel(X, Y) + 1\n",
    "        else:\n",
    "            self.kernel_function = kernel\n",
    "\n",
    "        if self.wu_bound == 'logistic':\n",
    "            self.cvx_kappa = lambda z: cp.logistic(z)\n",
    "            self.cvx_delta = lambda z: 1 - cp.logistic(-z)\n",
    "        elif self.wu_bound == 'hinge':\n",
    "            self.cvx_kappa = lambda z: cp.pos(1 + z)\n",
    "            self.cvx_delta = lambda z: 1 - cp.pos(1 - z)\n",
    "        elif self.wu_bound == 'squared':\n",
    "            self.cvx_kappa = lambda z: cp.square(1 + z)\n",
    "            self.cvx_delta = lambda z: 1 - cp.square(1 - z)\n",
    "        elif self.wu_bound == 'exponential':\n",
    "            self.cvx_kappa = lambda z: cp.exp(z)\n",
    "            self.cvx_delta = lambda z: 1 - cp.exp(-z)\n",
    "        else:\n",
    "            print('Using default bound with hinge.')\n",
    "            self.cvx_kappa = lambda z: cp.pos(1 + z)\n",
    "            self.cvx_delta = lambda z: 1 - cp.pos(1 - z)\n",
    "\n",
    "        self.nmb_pts = len(self.s_train)\n",
    "        self.nmb_unprotected = np.sum(self.s_train == 1)\n",
    "        self.prob_unprot = self.nmb_unprotected / self.nmb_pts\n",
    "        self.prob_prot = 1 - self.prob_unprot\n",
    "\n",
    "        self.nmb_pos = np.sum(self.y_train == 1)\n",
    "        self.nmb_prot_pos = np.sum(self.y_train[self.s_train == -1] == 1)\n",
    "        self.prob_prot_pos = self.nmb_prot_pos / self.nmb_pos\n",
    "        self.prob_unprot_pos = 1 - self.prob_prot_pos\n",
    "\n",
    "        # Create weights that are necessary for the fairness constraint\n",
    "        if self.fairness_notion == 'DDP':\n",
    "            normalizer = self.nmb_pts\n",
    "            self.weight_vector = np.array(\n",
    "                [1.0 / self.prob_prot if self.s_train[i] == -1 else 1.0 / self.prob_unprot for i in range(len(self.s_train))]).reshape(-1,1)\n",
    "            self.weight_vector = (1 / normalizer) * self.weight_vector\n",
    "        elif self.fairness_notion == 'DEO':\n",
    "            normalizer = self.nmb_pos\n",
    "            self.weight_vector = np.array(\n",
    "                [1.0 / self.prob_prot_pos if self.s_train[i] == -1 else 1.0 / self.prob_unprot_pos for i in range(len(self.s_train))]).reshape(-1, 1)\n",
    "            self.weight_vector = 0.5 * (self.y_train.reshape(-1, 1) + 1) * self.weight_vector\n",
    "            self.weight_vector = (1 / normalizer) * self.weight_vector\n",
    "\n",
    "        # Choose random reasonable points\n",
    "        if self.reason_points <= 1:\n",
    "            self.reason_pts_index = list(range(int(self.nmb_pts * self.reason_points)))\n",
    "        else:\n",
    "            self.reason_pts_index = list(range(self.reason_points))\n",
    "        self.nmb_reason_pts = len(self.reason_pts_index)\n",
    "\n",
    "    def _construct_problem(self, bound='upper'):\n",
    "        \"\"\" Construct the cvxpy minimization problem.\n",
    "        It depends on the fairness regularizer chosen.\n",
    "        \"\"\"\n",
    "\n",
    "        # Variable to optimize\n",
    "        self.alpha_var = cp.Variable((len(self.reason_pts_index), 1))\n",
    "        # Parameter for Kernel Matrix\n",
    "        self.kernel_matrix = cp.Parameter(shape=(self.x_train.shape[0], len(self.reason_pts_index)))\n",
    "        self.fair_reg_cparam = cp.Parameter(nonneg=True)\n",
    "\n",
    "\n",
    "        # Form SVM with L2 regularization\n",
    "        if self.fairness_lambda == 0:\n",
    "            self.loss = cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + self.reg_beta * self.nmb_pts * cp.square(cp.norm(self.alpha_var, 2))\n",
    "        else:\n",
    "            sy_hat = cp.multiply(self.s_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var)\n",
    "\n",
    "            if self.fairness_regularizer == 'wu':\n",
    "                if bound == 'upper':\n",
    "                    fairness_relaxation = cp.sum(cp.multiply(self.weight_vector, self.cvx_kappa(sy_hat))) - 1\n",
    "                else:\n",
    "                    fairness_relaxation = -1 * cp.sum(cp.multiply(self.weight_vector, self.cvx_delta(sy_hat))) - 1\n",
    "\n",
    "\n",
    "            elif self.fairness_regularizer == 'linear':\n",
    "                if bound == 'upper':\n",
    "                    fairness_relaxation = cp.sum(cp.multiply(self.weight_vector, self.kernel_matrix @ self.alpha_var))\n",
    "                else:\n",
    "                    fairness_relaxation = -1 * cp.sum(cp.multiply(self.weight_vector, self.kernel_matrix @ self.alpha_var))\n",
    "\n",
    "            if self.reg_beta == 0:\n",
    "                self.loss = (1/self.nmb_pts) * cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + \\\n",
    "                                self.fair_reg_cparam * fairness_relaxation\n",
    "            else:\n",
    "                self.loss = (1 / self.nmb_pts) * cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + \\\n",
    "                            self.fair_reg_cparam * fairness_relaxation + self.reg_beta * cp.square(cp.norm(self.alpha_var, 2))\n",
    "\n",
    "        self.prob = cp.Problem(cp.Minimize(self.loss))\n",
    "\n",
    "    def _optimize(self):\n",
    "        \"\"\"Conduct the optimization of the created problem by using ECOS or SCS\n",
    "        with cvxpy. \n",
    "        \"\"\"\n",
    "\n",
    "        # Compute and initialize kernel matrix\n",
    "        self.K_sim = self.kernel_function(self.x_train, self.x_train[self.reason_pts_index])\n",
    "        self.kernel_matrix.value = self.K_sim\n",
    "        self.fair_reg_cparam.value = self.fairness_lambda\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            verbose = True\n",
    "        else:\n",
    "            verbose = False\n",
    "        if self.solver == 'SCS':\n",
    "            self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
    "        elif self.solver == 'ECOS':\n",
    "            try:\n",
    "                self.prob.solve(solver=cp.ECOS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
    "            except Exception as e:\n",
    "                self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
    "        if verbose:\n",
    "            print('status %s ' % self.prob.status)\n",
    "            print('value %s ' % self.prob.value)\n",
    "        self.coef_ = self.alpha_var.value.squeeze()\n",
    "\n",
    "    def compute_fairness_measures(self, y_predicted, y_true, sens_attr):\n",
    "        \"\"\"Compute value of demographic parity and equality of opportunity for given predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_predicted: numpy array\n",
    "            The predicted class labels of shape=(number_points,).\n",
    "        y_true: numpy array\n",
    "            The true class labels of shape=(number_points,).\n",
    "        sens_attr: numpy array\n",
    "            The sensitive labels of shape=(number_points,).\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        DDP: float\n",
    "            The difference of demographic parity.\n",
    "        DEO: float\n",
    "            The difference of equality of opportunity.\n",
    "        \"\"\"\n",
    "        positive_rate_prot = self.get_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
    "        positive_rate_unprot = self.get_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
    "        true_positive_rate_prot = self.get_true_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
    "        true_positive_rate_unprot = self.get_true_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
    "        DDP = positive_rate_unprot - positive_rate_prot\n",
    "        DEO = true_positive_rate_unprot - true_positive_rate_prot\n",
    "\n",
    "        return DDP, DEO\n",
    "\n",
    "    def get_positive_rate(self, y_predicted, y_true):\n",
    "        \"\"\"Compute the positive rate for given predictions of the class label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_predicted: numpy array\n",
    "            The predicted class labels of shape=(number_points,).\n",
    "        y_true: numpy array\n",
    "            The true class labels of shape=(number_points,).\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        pr: float\n",
    "            The positive rate.\n",
    "        \"\"\"\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
    "        pr = (tp+fp) / (tp+fp+tn+fn)\n",
    "        return pr\n",
    "\n",
    "    def get_true_positive_rate(self, y_predicted, y_true):\n",
    "        \"\"\"Compute the true positive rate for given predictions of the class label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_predicted: numpy array\n",
    "            The predicted class labels of shape=(number_points,).\n",
    "        y_true: numpy array\n",
    "            The true class labels of shape=(number_points,).\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        tpr: float\n",
    "            The true positive rate.\n",
    "        \"\"\"\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
    "        tpr = tp / (tp+fn)\n",
    "        return tpr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_env]",
   "language": "python",
   "name": "conda-env-phd_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
