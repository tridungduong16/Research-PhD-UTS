{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cvxpy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8418d48d5ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcvx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# from gurobipy import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moff_pol_eval_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cvxpy"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import cvxpy as cvx \n",
    "# from gurobipy import * \n",
    "from off_pol_eval_functions import * \n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import collections as matcoll\n",
    "from sklearn import svm\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(2)\n",
    "def multipage(filename, figs=None, dpi=200):\n",
    "    pp = PdfPages(filename)\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for fig in figs:\n",
    "        fig.savefig(pp, format='pdf')\n",
    "    pp.close()\n",
    "\n",
    "d = 10  # dimension of x \n",
    "n = 2000; \n",
    "mu_x = np.zeros(d); \n",
    "sigma_x = np.random.normal(size = (d,1))\n",
    "sigma_x += np.abs(np.min(sigma_x))+0.5\n",
    "sigma_x = np.multiply(sigma_x, np.eye(d))\n",
    "sigma_x /= 2# normalize covariances a little bit \n",
    "\n",
    "W = 1.5 #treatment effect\n",
    "# interact_x = 2\n",
    "white_noise_coef = 0.1\n",
    "\n",
    "\n",
    "# generate propensity model \n",
    "def real_prop(x, beta_prop): \n",
    "    T_SIG = 5\n",
    "    if len(x.shape) > 1: \n",
    "        n= x.shape[1]\n",
    "    else:\n",
    "        n= len(x)\n",
    "    return np.dot(x, beta_prop) + np.random.normal(size = (n,1))*T_SIG\n",
    "    # T is normally distributed conditional on covariates \n",
    "    \n",
    "# coefficient of treatment effect\n",
    "beta_cons = -5\n",
    "beta_x = np.random.normal(size = (d,1))\n",
    "# interaction term with treatment \n",
    "beta_x_T = np.random.normal(size = (d,1))*1.5\n",
    "\n",
    "# sparse interaction terms \n",
    "sparse_entries = np.random.choice(range(d),size =  int(round(0.7*d)),replace = False)\n",
    "beta_x_T[sparse_entries] = 0     \n",
    "sparse_entries = np.random.choice(range(d),size =  int(round(0.35*d)),replace = False)\n",
    "beta_x[sparse_entries] = 0  \n",
    "\n",
    "FREQ = 20 \n",
    "beta_x_quad_T = np.random.normal(size = (d,1))\n",
    "sparse_entries = np.random.choice(range(d),size =  int(round(0.65*d)),replace = False)\n",
    "beta_x_quad_T[sparse_entries] = 0\n",
    "\n",
    "TRUE_PROP_BETA = np.asarray(beta_x_quad_T + np.random.normal( loc= np.ones((d,1))*2, size = (d,1))).flatten()\n",
    "print TRUE_PROP_BETA\n",
    "def real_risk(T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x):    \n",
    "    n = len(T); risk = np.zeros(n)\n",
    "    if np.isscalar(T):\n",
    "        risk = T*beta_cons + np.dot(beta_x.T, x) + np.dot(beta_x_T.T, x*T) + (T-np.dot(beta_x_quad_T.T,x))**2\n",
    "    else: \n",
    "        for i in range(len(T)): \n",
    "            risk[i] = T[i]*beta_cons + np.dot(beta_x.T, x[i,:]) + np.dot(beta_x_T.T, x[i,:]*T[i]) + (T[i]-np.dot(beta_x_quad_T.T,x[i,:]))**2#+ np.dot(beta_x_quad_T.T, (x[i,:]**2)*T[i]) + np.dot(beta_x_high_freq.T, np.sin(x[i,0:HIGH_FREQ_N]*FREQ)*T[i])\n",
    "    return risk\n",
    "\n",
    "T_SIG = 4\n",
    "def generate_data(mu_x, sigma_x_mat, n, beta_cons, beta_x, beta_x_T): \n",
    "#     x = np.random.normal(mu_x, sigma_x, size = n)\n",
    "    # generate n datapoints from the same multivariate normal distribution\n",
    "    x = np.random.multivariate_normal(mean = mu_x, cov= sigma_x_mat, size = n ) \n",
    "    print x.shape \n",
    "    print \"xshape\"\n",
    "    T = np.random.normal(0, T_SIG, n) + np.dot(x, TRUE_PROP_BETA) + 2*x[:,1] + 4*x[:,2] - 2*x[:,3]\n",
    "    true_resid = T - np.dot(x, TRUE_PROP_BETA)\n",
    "    true_Q = norm.pdf( T - np.dot(x, TRUE_PROP_BETA), loc = 0, scale = T_SIG )\n",
    "    y_sigma = 0.5\n",
    "    white_noise_coef = 5\n",
    "    \n",
    "    clf = LinearRegression(); clf.fit(x, T)\n",
    "    y_hat = clf.predict(x)\n",
    "    Y = np.zeros(n)\n",
    "    for i in range(n): \n",
    "        Y[i] = T[i]*beta_cons + np.dot(beta_x.T, x[i,:]) + T[i]*np.dot(beta_x_T.T, x[i,:]) + (T[i] - np.dot(beta_x_quad_T.T,x[i,:]))**2 #+ np.dot(beta_x_quad_T.T, (x[i,:]**2)*T[i]) + np.dot(beta_x_high_freq.T, np.sin(x[i,0:HIGH_FREQ_N]*FREQ)*T[i])\n",
    "    Y += np.random.multivariate_normal(mean = np.zeros(n), cov=white_noise_coef * np.eye(n))\n",
    "    # get pdf from residuals \n",
    "    resid = Y - y_hat\n",
    "    # get norm pdf \n",
    "    Q = norm.pdf(resid, loc = np.mean(resid), scale=np.std(resid))\n",
    "    T = T.flatten()\n",
    "    return [x, T, Y, true_Q, clf]\n",
    "\n",
    "[x_full, T_full, Y_full, true_Q_full, clf] = generate_data(mu_x, sigma_x, n, beta_cons, beta_x, beta_x_T)\n",
    "\n",
    "#compute real risk \n",
    "print np.mean( real_risk(T_full, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_full))\n",
    "\n",
    "plt.hist(Y_full); plt.title('Y')\n",
    "plt.figure()\n",
    "plt.title('T')\n",
    "plt.hist(T_full)\n",
    "Q = true_Q_full\n",
    "plt.figure()\n",
    "plt.hist(true_Q_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this setting the optimal treatment is similar to the observed treatment policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 200\n",
    "x_full=x_full[Y_full < cutoff]\n",
    "T_full=T_full[Y_full < cutoff]\n",
    "true_Q_full=true_Q_full[Y_full < cutoff]\n",
    "Y_full = Y_full[Y_full < cutoff]\n",
    "\n",
    "plt.hist(Y_full); plt.title('Y')\n",
    "plt.figure()\n",
    "plt.title('T')\n",
    "plt.hist(T_full)\n",
    "Q = true_Q_full\n",
    "plt.figure()\n",
    "plt.hist(true_Q_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reserve half for out of sample evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x_test = x_full[n:]; Y_test = Y_full[n:]; T_test = T_full[n:]; true_Q = true_Q_full[n:]\n",
    "x = x_full[0:n]; Y = Y_full[0:n];T = T_full[0:n]; true_Q = true_Q_full[0:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal treatment tends to bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pickle\n",
    "\n",
    "# pickle.dump(x_full, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) + 'x.p', 'wb'))\n",
    "# pickle.dump(T_full, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) + 'T.p', 'wb'))\n",
    "# pickle.dump(Y_full, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) + 'Y.p', 'wb'))\n",
    "# pickle.dump(true_Q_full, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) + 'Q.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_T = np.linspace(-15,15,20)\n",
    "\n",
    "plt.scatter(cons_T,  [np.mean( real_risk(np.ones(n)*t, beta_cons, beta_x, beta_x_T, beta_x_quad_T,x)) for t in cons_T ] )\n",
    "\n",
    "def real_risk(T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x):    \n",
    "    if np.isscalar(T):\n",
    "        risk = T*beta_cons + np.dot(beta_x.T, x) + np.dot(beta_x_T.T, x*T) + (T-np.dot(beta_x_quad_T.T,x))**2\n",
    "    else: \n",
    "        risk = np.zeros(len(T))\n",
    "        for i in range(len(T)): \n",
    "            risk[i] = T[i]*beta_cons + np.dot(beta_x.T, x[i,:]) + np.dot(beta_x_T.T, x[i,:]*T[i])+ (T[i]-np.dot(beta_x_quad_T.T,x[i,:]))**2 #+ np.dot(beta_x_quad_T.T, (x[i,:]**2)*T[i]) + np.dot(beta_x_high_freq.T, np.sin(x[i,0:HIGH_FREQ_N]*FREQ)*T[i])\n",
    "    return risk\n",
    "def real_risk_scalar(T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x):    \n",
    "    return T*beta_cons + np.dot(beta_x.T, x) + np.dot(beta_x_T.T, x*T) + (T-np.dot(beta_x_quad_T.T,x))**2\n",
    "\n",
    "bounds = [(min(T)/(0.5*d) , max(T)/np.abs(0.5*d)  ) for i in range(d) ]\n",
    "bnds = tuple(tuple(x) for x in bounds)\n",
    "\n",
    "def get_oracle_treatment(beta_cons, beta_x, beta_x_T, beta_x_quad_T, x): \n",
    "    res = minimize(real_risk_scalar, np.mean(T), args =(beta_cons, beta_x, beta_x_T, beta_x_quad_T,x))\n",
    "    return res.x \n",
    "\n",
    "oracle_T = [ get_oracle_treatment(beta_cons, beta_x, beta_x_T, beta_x_quad_T, x[i,:]) for i in range(n)]\n",
    "plt.hist(np.asarray(oracle_T))\n",
    "\n",
    "fig=plt.figure()\n",
    "for i in range(50): \n",
    "    plt.plot(cons_T, np.asarray([ real_risk(t, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x[i,:]) for t in cons_T ]), alpha = 0.3 )\n",
    "\n",
    "for i in range(50): \n",
    "    plt.scatter(T[i], real_risk(T[i], beta_cons, beta_x, beta_x_T, beta_x_quad_T, x[i,:]),color='r')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('y (loss)')\n",
    "# plt.axis('off')\n",
    "\n",
    "\n",
    "fig.savefig('outcome-model.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_risk(disc_res,  )\n",
    "# disc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = -3\n",
    "\n",
    "def pol_eval_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    data['tau'] = tau\n",
    "    return data['sgn']*(off_policy_evaluation(**data))\n",
    "\n",
    "def pol_eval_l2_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    data['tau'] = tau\n",
    "    return data['sgn']*(off_policy_evaluation(**data) + LAMBDA * np.linalg.norm(beta,2))\n",
    "\n",
    "\n",
    "def pol_eval_l1_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    data['tau'] = tau\n",
    "    return data['sgn']*(off_policy_evaluation(**data) + LAMBDA * sum(abs(beta)))\n",
    "\n",
    "def off_policy_evaluation(**params):\n",
    "    \"\"\"\n",
    "    Takes in a choice of kernel and dictionary of parameters and data required for evaluation\n",
    "    tau is a vector of treatment values (assumed given)\n",
    "    If y_samp, T_samp is present, use that instead. \n",
    "    \"\"\"\n",
    "    THRESH = params['threshold']\n",
    "    y_out = params['y']; x = params['x']; h = params['h'];Q = params['Q']; n = params['n']; t_lo = params['t_lo'];  t_hi = params['t_hi']\n",
    "    kernel = params['kernel_func'];kernel_int =  params['kernel_int_func']\n",
    "    if ('y_samp' in params.keys()):\n",
    "        y_out = params['y_samp']\n",
    "    if ('T_samp' in params.keys()): \n",
    "        T = params['T_samp']\n",
    "    else: \n",
    "        T = params['T']\n",
    "    if ('x_samp' in params.keys()):\n",
    "        x = params['x_samp']\n",
    "    loss = 0\n",
    "    tau = params['tau']\n",
    "    clip_tau = np.clip(tau, t_lo, t_hi)\n",
    "    for i in np.arange(n): \n",
    "        Q_i = Q(x[i,:], T[i], t_lo, t_hi)\n",
    "        if (abs(clip_tau[i] - t_lo) <= h):\n",
    "            alpha = kernel_int((t_lo-clip_tau[i])/h, 1)\n",
    "            if alpha < 0.5: \n",
    "                print alpha\n",
    "        elif (abs(clip_tau[i] - t_hi) <= h):\n",
    "            alpha = kernel_int(-1,  (t_hi - clip_tau[i])/h )\n",
    "            if alpha < 0.5: \n",
    "                print alpha\n",
    "        else:\n",
    "            alpha = 1\n",
    "        loss += kernel( (clip_tau[i] - T[i])/h )*1.0 * y_out[i]/max(Q_i,THRESH) * 1.0/alpha\n",
    "    return loss/(1.0*h*n)\n",
    "\n",
    "\n",
    "def off_pol_epan_lin_l2_grad(beta, *args):\n",
    "    \"\"\"\n",
    "    Compute a gradient for special case of Epanechnikov kernel and linear policy tau\n",
    "    \"\"\"\n",
    "    # THRESH = 0.001\n",
    "    d = len(beta) \n",
    "    params = dict(args[0])\n",
    "    #! FIXME x vs xsamp\n",
    "    tau = np.dot(beta, params['x'].T)\n",
    "    params['tau'] = tau\n",
    "    params['beta'] = beta\n",
    "\n",
    "    THRESH = params['threshold']\n",
    "\n",
    "    [f, g, nabla_f, nabla_g] = f_g(**params)\n",
    "    # compute gradient vector via quotient rule\n",
    "    if g < THRESH: \n",
    "        g = THRESH  \n",
    "    \n",
    "    return params['sgn']*(np.asarray((g*nabla_f - f*nabla_g) / g**2 ) + LAMBDA * 2*beta)\n",
    "\n",
    "def off_pol_epan_lin_l1_grad(beta, *args):\n",
    "    \"\"\"\n",
    "    Compute a gradient for special case of Epanechnikov kernel and linear policy tau\n",
    "    \"\"\"\n",
    "    d = len(beta) \n",
    "    params = dict(args[0])\n",
    "    #! FIXME x vs xsamp\n",
    "    tau = np.dot(beta, params['x'].T)\n",
    "    params['tau'] = tau; params['beta'] = beta\n",
    "    THRESH = params['threshold']\n",
    "    [f, g, nabla_f, nabla_g] = f_g(**params)\n",
    "    # compute gradient vector via quotient rule\n",
    "    if g < THRESH: \n",
    "        g = THRESH  \n",
    "    l1_sub = np.zeros(d)\n",
    "    for i in range(d): \n",
    "        if beta[i] < 0: \n",
    "            l1_sub[i] = -1 \n",
    "        elif beta[i] > 0: \n",
    "            l1_sub[i] = 1\n",
    "        else: \n",
    "            l1_sub[i] = 0 \n",
    "    return params['sgn']*(np.asarray((g*nabla_f - f*nabla_g) / g**2 ) + LAMBDA * l1_sub)\n",
    "\n",
    "def f_g(**params): \n",
    "    THRESH = params['threshold']\n",
    "    y_out = params['y']; x = params['x']; h = params['h'];Q = params['Q']; n = params['n']; t_lo = params['t_lo'];  t_hi = params['t_hi']\n",
    "    kernel = params['kernel_func'];kernel_int =  params['kernel_int_func']\n",
    "    if ('y_samp' in params.keys()):\n",
    "        y_out = params['y_samp']\n",
    "    if ('T_samp' in params.keys()): \n",
    "        T = params['T_samp']\n",
    "    else: \n",
    "        T = params['T']\n",
    "    if ('x_samp' in params.keys()):\n",
    "        x = params['x_samp']\n",
    "    BMI_IND = params.get('BMI_IND') # propensity score for warfarin data evaluations \n",
    "        \n",
    "    loss = 0\n",
    "    g = 0 # also keep track of normalized probability ratio quantity \n",
    "    partial_f = 0 \n",
    "    partial_g = 0 \n",
    "    tau = params['tau']\n",
    "    clip_tau = np.clip(tau, t_lo, t_hi)\n",
    "    Qs = np.zeros(n)\n",
    "    for i in np.arange(n): \n",
    "        if (params.get('DATA_TYPE') == 'warfarin'): \n",
    "            Q_i = Q(x[i,BMI_IND], T[i], t_lo, t_hi)\n",
    "        else: \n",
    "            Q_i = Q(x[i], T[i], t_lo, t_hi)\n",
    "        if (abs(clip_tau[i] - t_lo) <= h):\n",
    "            alpha = kernel_int((t_lo-clip_tau[i])/h, 1)\n",
    "        elif (abs(clip_tau[i] - t_hi) <= h):\n",
    "            alpha = kernel_int(-1,  (t_hi - clip_tau[i])/h )\n",
    "        else:\n",
    "            alpha = 1\n",
    "        Qs[i] = kernel( (clip_tau[i] - T[i])/h )/max(Q_i,THRESH)\n",
    "        loss += kernel( (clip_tau[i] - T[i])/h )*1.0 * y_out[i]/max(Q_i,THRESH) * 1.0/alpha\n",
    "        if abs((clip_tau[i] - T[i])/h) >= 1:\n",
    "            partial_f += 0 # don't add anything to partial derivatives \n",
    "        else:\n",
    "            partial_g += -1.5 * ((clip_tau[i] - T[i])/h ) * 1.0/max(Q_i,THRESH) * x[i,:]\n",
    "            partial_f += -1.5 * ((clip_tau[i] - T[i])/h ) * y_out[i]/max(Q_i,THRESH) * x[i,:]\n",
    "    norm_sum = np.mean(Qs)\n",
    "    return [loss/(1.0*h*n), 1.0*norm_sum/h, partial_f/(1.0*n*h**2) , partial_g/(1.0*n*h**2) ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_eval_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    x = data['x']\n",
    "    return np.mean(real_risk(tau, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train GPS on your simulated data\n",
    "def norm_T_Q(x,t,t_lo,t_hi): \n",
    "    return norm.pdf( t - np.dot(x, TRUE_PROP_BETA), loc = 0, scale = T_SIG )\n",
    "\n",
    "trainind = np.random.choice(range(n),size = int(round(0.4*n)),replace = False)\n",
    "train = x[trainind]\n",
    "test_mask = np.ones(n, dtype=bool)\n",
    "test_mask[trainind] = False\n",
    "test = x[test_mask,:]\n",
    "t_lo = min(T)\n",
    "t_hi = max(T) \n",
    "d = x.shape[1]\n",
    "train_data = { 'n': train.shape[0],'h': 4, 'y': Y[trainind],'Q': norm_T_Q,'x_full': x[trainind], 'x': x[trainind],'x_samp': x[trainind], 'T_samp': T[trainind], 'd': d, 'T': T[trainind],'t_lo': t_lo ,'t_hi': t_hi  }\n",
    "train_data['kernel_int_func'] = epanechnikov_int\n",
    "train_data['T_samp'] = np.array(T[trainind],order='F'); train_data['y_samp'] = Y[trainind]\n",
    "train_data['kernel_func'] = epanechnikov_kernel\n",
    "train_data['inds'] = trainind\n",
    "train_data['threshold'] = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretize_tau_policy(**params):\n",
    "    '''\n",
    "    Discretize the treatment vector 'tau' according to uniform binning.\n",
    "    '''\n",
    "    x = params['x_samp']\n",
    "    T = params['T_samp']\n",
    "    n_bins = params['n_bins']\n",
    "    t_lo = min(T)\n",
    "    t_hi = max(T)\n",
    "    bins = np.linspace(t_lo, t_hi, n_bins)\n",
    "    T_binned = np.digitize(T, bins).flatten()\n",
    "    bin_means = [T[T_binned == i].mean() for i in range(1, n_bins)]\n",
    "    tau_binned = np.digitize(params['tau'], bins).flatten()\n",
    "    return tau_binned\n",
    "\n",
    "def off_pol_disc_evaluation(policy, **params):\n",
    "    THRESH = params['threshold']\n",
    "    y_out = params['y']; x = params['x_samp']; h = params['h']; Q = params['Q']; n = params['n']; t_lo = params['t_lo']; t_hi = params['t_hi']\n",
    "    n_bins = params['n_bins']\n",
    "    if ('y_samp' in params.keys()):\n",
    "        y_out = params['y_samp'].flatten()\n",
    "    if ('T_samp' in params.keys()):\n",
    "        T = params['T_samp'].flatten()\n",
    "    else:\n",
    "        T = params['T'].flatten()\n",
    "\n",
    "    t_lo = min(T)\n",
    "    t_hi = max(T)\n",
    "    bin_width = t_hi-t_lo\n",
    "    bins = np.linspace(t_lo, t_hi, n_bins)\n",
    "    T_binned = np.digitize(T, bins, right = True).flatten()\n",
    "    bin_means = [T[T_binned == i].mean() for i in range(1, len(bins))]\n",
    "\n",
    "    loss = 0\n",
    "    tau_vec = policy(**params).flatten()\n",
    "\n",
    "    i=0\n",
    "    #! FIXME need to establish whether policy returns discrete bins or means\n",
    "    treatment_overlap = np.where(np.equal(tau_vec.flatten(), T_binned))[0]\n",
    "    n_overlap = len(treatment_overlap)\n",
    "    Qs = np.zeros(n_overlap)\n",
    "    for ind in treatment_overlap:\n",
    "        Q_i = Q(x[ind], bin_means[T_binned[ind]-1], t_lo, t_hi) * bin_width*1.0/n_bins # BUG FIX: this is going to have to be integrated against \n",
    "        Qs[i] = 1.0/max(Q_i,THRESH)\n",
    "        loss += y_out[ind]/max(Q_i,THRESH)\n",
    "        i+=1 \n",
    "    \n",
    "    norm_sum = np.mean(Qs)\n",
    "    if n_overlap == 0:\n",
    "        print \"no overlap\"\n",
    "        return 0\n",
    "    return loss/(1.0*n*norm_sum)\n",
    "\n",
    "def pol_disc_l2_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']; pol = data['pol']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    data['tau'] = tau\n",
    "    return data['sgn']*(off_pol_disc_evaluation(pol,**data) + LAMBDA * np.linalg.norm(beta,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lo = min(T)\n",
    "t_hi = max(T)\n",
    "bins = np.linspace(t_lo, t_hi, n_bins)\n",
    "T_binned = np.digitize(T, bins).flatten()\n",
    "bin_means = [T[T_binned == i].mean() for i in range(1, n_bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'n': nn,'h': 5*pow((n_0*1.0/nn),.2), 'y': Y[trainind],'Q': norm_T_Q,'x_full': x[trainind,:], 'x': x[trainind,:],'x_samp': x[trainind,:], 'T_samp': T[trainind].flatten(), 'd': d, 'T': T[trainind],'t_lo': t_lo ,'t_hi': t_hi  }\n",
    "data['kernel_int_func'] = epanechnikov_int\n",
    "data['T_samp'] = np.array(T[trainind],order='F'); train_data['y_samp'] = Y[trainind]\n",
    "data['kernel_func'] = epanechnikov_kernel\n",
    "data['inds'] = trainind; data['sgn'] = 1\n",
    "data['threshold'] = 0.02\n",
    "data['n_bins'] = 10 \n",
    "data['pol'] = discretize_tau_policy\n",
    "data['tau'] = T[trainind]\n",
    "off_pol_disc_evaluation(discretize_tau_policy, **data)\n",
    "beta_d = np.random.uniform(size=(d,1))*4\n",
    "disc_res = minimize(pol_disc_l2_wrapper, x0 = beta_d, bounds = bnds, options={'disp':True,'gtol':1e-1,'maxfun': 1000}, args=data.items())\n",
    "data['tau'] = np.dot(disc_res.x, x.T)\n",
    "disc_pol = discretize_tau_policy(**data)\n",
    "disc_T= np.dot(disc_res.x, x.T)\n",
    "np.mean( real_risk(np.dot(disc_res.x, x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "np.random.seed(2)\n",
    "\n",
    "n = len(Y)\n",
    "SAMP_N = 5; \n",
    "ns = [200, 400]#, 800, 1600 ]\n",
    "n_0 = ns[0]\n",
    "cons_evals = np.zeros([len(ns), SAMP_N]); mean_evals = np.zeros([len(ns), SAMP_N])\n",
    "zero_evals = np.zeros([len(ns), SAMP_N]); oracle_evals = np.zeros([len(ns), SAMP_N]);DR_oracle_evals = np.zeros([len(ns), SAMP_N])\n",
    "oracle_coefs = [ [None]*SAMP_N ] * len(ns)\n",
    "DR_oracle_coefs = [ [None]*SAMP_N ] * len(ns)\n",
    "bounds = [(min(T)/(0.5*d) , max(T)/np.abs(0.5*d)  ) for i in range(d) ]\n",
    "bnds = tuple(tuple(x) for x in bounds)\n",
    "# bounds = [(-np.mean(sampled_T)/max(np.mean(X_sim_std[:,i]), 0.25*d) , np.mean(sampled_T)/max(np.mean(X_sim_std[:,i]),d*.25) ) for i in range(d) ]\n",
    "# bounds = [(-max(sim_dose)/(.25*d*np.mean(subframe[:,i])) , max(sim_dose)/(.25*d*np.mean(subframe[:,i])) ) for i in range(d) ]\n",
    "print \"opt coef bounds for beta: \"\n",
    "print bounds\n",
    "\n",
    "n_restarts = 1\n",
    "for ind, nn in enumerate(ns): \n",
    "    print nn \n",
    "    for k in range(SAMP_N): \n",
    "        trainind = np.random.choice(range(n),size = nn, replace = False)\n",
    "        sub_T = np.zeros(nn)\n",
    "        \n",
    "        # Learn GPS from data\n",
    "        lr = LinearRegression(); lr.fit(x[trainind,:], T[trainind])\n",
    "        T_hat = lr.predict(x[trainind,:])\n",
    "        beta_T_gps = np.dot(x[trainind,:],lr.coef_)\n",
    "        resid = T[trainind] - T_hat\n",
    "        # Assume normal noise\n",
    "        mu_resid = np.mean(resid); sigma_resid = np.std(resid)\n",
    "        def norm_T_Q_est(x,t,t_lo,t_hi): \n",
    "            return norm.pdf( t - np.dot(x, lr.coef_), loc =  mu_resid, scale = sigma_resid )\n",
    "        \n",
    "        data = { 'n': nn,'h': 3*pow((n_0*1.0/nn),.2), 'y': Y[trainind],'Q': norm_T_Q_est,'x_full': x[trainind,:], 'x': x[trainind,:],'x_samp': x[trainind,:], 'T_samp': T[trainind].flatten(), 'd': d, 'T': T[trainind],'t_lo': t_lo ,'t_hi': t_hi  }\n",
    "        data['kernel_int_func'] = epanechnikov_int\n",
    "        data['T_samp'] = np.array(T[trainind],order='F'); train_data['y_samp'] = Y[trainind]\n",
    "        data['kernel_func'] = epanechnikov_kernel\n",
    "        data['inds'] = trainind; data['sgn'] = 1\n",
    "        data['threshold'] = 0.02\n",
    "        betas = []; vals = []\n",
    "        \n",
    "        k_ind = 0\n",
    "        while k_ind <= n_restarts: \n",
    "            k_ind += 1\n",
    "            beta_d = np.random.uniform(size=d)\n",
    "            LAMBDA = 0.5\n",
    "            oracle_res = minimize(pol_eval_l2_wrapper, x0 = beta_d, jac = off_pol_epan_lin_l2_grad, bounds = bnds, options={'disp':True,'gtol':1e-1,'maxfun': 1000}, args=data.items())  \n",
    "            betas += [oracle_res.x]\n",
    "            print oracle_res.x\n",
    "            vals += [oracle_res.fun]\n",
    "            print oracle_res.fun\n",
    "        oracle_coefs[ind][k] = betas[np.argmin(vals)] \n",
    "        oracle_evals[ind, k] = vals[np.argmin(vals)] \n",
    "        zero_T = np.zeros(nn); data['tau'] = zero_T; zero_evals[ind,k] = off_policy_evaluation(**data)\n",
    "#         oracle_coefs[ind] += [ betas[np.argmin(vals)] ]\n",
    "#         oracle_evals[ind, k] = vals[np.argmin(vals)]\n",
    "        mu_T = np.mean(T)*np.ones(nn); data['tau'] = mu_T; mean_evals[ind,k] = off_policy_evaluation(**data)\n",
    "        \n",
    "        clf = RandomForestRegressor(n_estimators=5)\n",
    "        clf = clf.fit(x[trainind,:],y=Y[trainind])\n",
    "#         data['rf'] = clf\n",
    "        y_hat = clf.predict(x[trainind,:])\n",
    "        data['y'] = Y[trainind] - y_hat\n",
    "        betas = []; vals = []\n",
    "        k_ind = 0\n",
    "        while k_ind <= n_restarts: \n",
    "            k_ind += 1\n",
    "            beta_d = np.random.uniform(size=d)\n",
    "            LAMBDA = 0.5\n",
    "            oracle_res = minimize(pol_eval_l2_wrapper, x0 = beta_d, jac = off_pol_epan_lin_l2_grad, bounds = bnds, options={'disp':True,'gtol':1e-1,'maxfun': 1000}, args=data.items())  \n",
    "            print oracle_res.x\n",
    "            betas += [oracle_res.x]\n",
    "            vals += [oracle_res.fun]\n",
    "            print oracle_res.fun\n",
    "        DR_oracle_coefs[ind][k] = betas[np.argmin(vals)] \n",
    "        DR_oracle_evals[ind, k] = vals[np.argmin(vals)] \n",
    "        \n",
    "        data['n_bins'] = 10 \n",
    "        off_pol_disc_evaluation(discretize_tau_policy, **data)\n",
    "        \n",
    "        ## Best-in-class policy via regression\n",
    "        # find best in class policy \n",
    "#         d = x.shape[1]; beta = cvx.Variable(d)\n",
    "#         # Implement a thresholded loss\n",
    "#         cost = cvx.sum_entries(cvx.square(x[trainind,:]*beta)) # minimize y^2 \n",
    "#         constraints = [beta >=min(T)/(0.5*d) ]\n",
    "#         for i in range(d): \n",
    "#             constraints += [beta[i] <= max(T)/(0.5*d) ]\n",
    "#         prob = cvx.Problem(cvx.Minimize(cost), constraints)\n",
    "#         prob.solve('SCS')\n",
    "#         print beta.value\n",
    "#         prescient_beta = beta.value.flatten().T\n",
    "#         prescient_beta = np.ravel(prescient_beta).T\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\beta^T X + \\beta_{x,T}*X\\circ T + (\\beta_{x,T^2}^T x - T)^2$$\n",
    "\n",
    "Suppose we replace T with the linear policy $\\beta_{\\tau}x$: \n",
    "\n",
    "$$ \\beta^T X + \\beta_{x,T}*X\\circ \\beta_{\\tau}X + ((\\beta_{x,T^2} - \\beta_{\\tau})^T X)^2 $$ \n",
    "\n",
    "If we derive wrt $\\beta_{\\tau x}$: \n",
    "\n",
    "$$ \\beta_{x,T}X +2(\\beta_{x,T^2}^T X - \\beta_{\\tau}X)X = 0 $$\n",
    "\n",
    "$$\\beta_{\\tau}^T X)X =  \\beta_{x,T}X +2\\beta_{x,T^2}^T X $$\n",
    "\n",
    "$$\\beta_{\\tau}^T  =  (\\beta_{x,T}X +2\\beta_{x,T^2}^T X)(X^TX)^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_eval_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = np.clip(np.dot(beta, data['x'].T) , t_lo, t_hi).flatten()\n",
    "    x = data['x']\n",
    "    return np.mean(real_risk(np.dot(tau, x), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x))\n",
    "beta_d = np.random.uniform(size=(d,1))*5\n",
    "best_in_class = minimize(oracle_eval_wrapper, x0 = beta_d, bounds = bnds, options={'disp':True,'gtol':1e-1,'maxfun': 1000}, args=data.items())  \n",
    "print best_in_class.x\n",
    "print best_in_class.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_risk(T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x):    \n",
    "    n = len(T); risk = np.zeros(n)\n",
    "    if np.isscalar(T):\n",
    "        risk = T*beta_cons + np.dot(beta_x.T, x) + np.dot(beta_x_T.T, x*T) + (T-np.dot(beta_x_quad_T.T,x))**2\n",
    "    else: \n",
    "        for i in range(len(T)): \n",
    "            risk[i] = T[i]*beta_cons + np.dot(beta_x.T, x[i,:]) + np.dot(beta_x_T.T, x[i,:]*T[i]) + (T[i]-np.dot(beta_x_quad_T.T,x[i,:]))**2#+ np.dot(beta_x_quad_T.T, (x[i,:]**2)*T[i]) + np.dot(beta_x_high_freq.T, np.sin(x[i,0:HIGH_FREQ_N]*FREQ)*T[i])\n",
    "    return risk\n",
    "\n",
    "zero_T = np.zeros(n); \n",
    "# pred_T = np.dot(oracle_coefs[1][0], x.T)\n",
    "dr_T = np.dot(DR_oracle_coefs[1][0], x.T)\n",
    "colors = ['b', 'g', 'r', 'y','orange']\n",
    "EVALS = [zero_evals, mean_evals, DR_oracle_evals]\n",
    "\n",
    "# use scipy minimize (1D optimize outcome wrt T )\n",
    "def get_oracle_treatment(beta_cons, beta_x, beta_x_T, x): \n",
    "    res = minimize(real_risk, np.mean(T), args =(beta_cons, beta_x, beta_x_T, beta_x_quad_T, x))\n",
    "    return res.x \n",
    "\n",
    "POLICIES = [ zero_T, mu_T, dr_T ]\n",
    "lbls = ['zero treatment policy', 'mean treatment policy', 'DR treatment policy'   ]\n",
    "\n",
    "def plot_evals(evals, POLICY, X, c, lbl, ns, SAMP_N): \n",
    "    plt.axhline(y = Y_OFFSET + np.mean(real_risk(POLICY, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)),color=c)\n",
    "    plt.scatter(ns, np.mean(evals,axis=1),label=lbl,color=c)\n",
    "    error = 1.96*np.std(evals,axis=1)/np.sqrt(SAMP_N)\n",
    "    plt.fill_between(ns, np.mean(evals,axis=1)-error,np.mean(evals,axis=1)+error, alpha=0.3, edgecolor=c, facecolor=c)\n",
    "# plt.axhline(y = np.mean(real_risk(oracle_T beta_cons, beta_x, beta_x_T, x)))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for j in range(len(ns)):\n",
    "    for i in range(SAMP_N): \n",
    "        plt.hist( np.asarray(real_risk(np.dot(DR_oracle_coefs[j][i], x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)).flatten())\n",
    "\n",
    "plt.figure()\n",
    "[ plot_evals(EVALS[i], POLICIES[i], x, colors[i], lbls[i], ns, SAMP_N) for i in range(len(EVALS))]\n",
    "for j in range(len(ns)):\n",
    "    for i in range(SAMP_N): \n",
    "        plt.axhline(y = np.mean(real_risk(np.dot(oracle_coefs[j][i], x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)),color='r')\n",
    "    for i in range(SAMP_N): \n",
    "        plt.axhline(y = np.mean(real_risk(np.dot(DR_oracle_coefs[j][i], x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)),color='r')\n",
    "plt.axhline(y = np.mean(real_risk(np.dot(oracle_res.x, x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(real_risk(np.dot(oracle_res.x, x.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x))\n",
    "# plt.axhline(y = np.mean(real_risk(pred_T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)),color=c)\n",
    "plt.figure()\n",
    "plt.hist(real_risk(zero_T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x))\n",
    "\n",
    "\n",
    "# def plot_evals(evals, POLICY, X, c, lbl, ns, SAMP_N): \n",
    "#     plt.axhline(y = np.mean(real_risk(POLICY, beta_cons, beta_x, beta_x_T, x)),color=c)\n",
    "#     plt.scatter(ns, np.mean(evals,axis=1),label=lbl,color=c)\n",
    "#     error = 1.96*np.std(evals,axis=1)/np.sqrt(SAMP_N)\n",
    "#     plt.fill_between(ns, np.mean(evals,axis=1)-error,np.mean(evals,axis=1)+error, alpha=0.5, edgecolor=c, facecolor=c)\n",
    "# plt.axhline(y = np.mean(real_risk(oracle_T, beta_cons, beta_x, beta_x_T, x)))\n",
    "# [ plot_evals(EVALS[i], POLICIES[i], x, colors[i], lbls[i], ns, SAMP_N) for i in range(len(EVALS))]\n",
    "\n",
    "import pickle\n",
    "plt.figure()\n",
    "for i in range(len(EVALS)):  \n",
    "    pickle.dump(EVALS[i], open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) +lbls[i]+ 'EVALS.p', 'wb'))\n",
    "    pickle.dump(POLICIES[i], open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) +lbls[i]+ 'policy.p', 'wb'))\n",
    "    pickle.dump(oracle_coefs, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) +lbls[i]+ 'oracle_coefs.p', 'wb'))\n",
    "    pickle.dump(DR_oracle_coefs, open(str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")) +lbls[i]+ 'DR_oracle_coefs.p', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute POEM \n",
    "import sys\n",
    "sys.path.append('./POEM-norm/')\n",
    "import DatasetReader, Skylines\n",
    "\n",
    "def norm_T_Q(x,t,t_lo,t_hi): \n",
    "    return norm.pdf( t - np.dot(x, TRUE_PROP_BETA), loc = 0, scale = T_SIG )\n",
    "\n",
    "t_lo = min(T)\n",
    "t_hi = max(T)\n",
    "bins = np.linspace(t_lo, t_hi, n_bins)\n",
    "bin_width = (t_hi-t_lo)*1.0/n_bins\n",
    "T_binned = np.digitize(T, bins,right=True).flatten()\n",
    "bin_means = [T[T_binned == i].mean() for i in range(1, n_bins)]\n",
    "Xtrain = x[trainind,:]\n",
    "ttrain = T_binned[trainind]\n",
    "\n",
    "gps = np.asarray([bin_width*norm_T_Q(x[i,:], bin_means[ttrain[i]-1], 0.5, 0.5 ) for i in range(len(trainind))])\n",
    "# all 0-1\n",
    "# gps = integrated_propensity_scores\n",
    "ytrain = Y[trainind]\n",
    "Xtest = x\n",
    "\n",
    "mydata = DatasetReader.BanditDataset(None,False)\n",
    "mydata.trainFeatures        = np.hstack((Xtrain.copy(),np.ones((len(Xtrain),1))))\n",
    "mydata.sampledLabels        = np.zeros((len(ttrain),max(ttrain)+1))\n",
    "mydata.sampledLabels[range(len(ttrain)),ttrain] = 1.\n",
    "mydata.trainLabels          = np.empty(mydata.sampledLabels.shape)\n",
    "mydata.sampledLoss          = ytrain.copy()\n",
    "mydata.sampledLoss         -= mydata.sampledLoss.min()\n",
    "mydata.sampledLoss         /= mydata.sampledLoss.max()\n",
    "# computed on training set \n",
    "mydata.sampledLogPropensity = np.log(gps)\n",
    "#ones_like vs ones_line? \n",
    "mydata.testFeatures              = np.hstack((np.ones_like(Xtest),np.ones((len(Xtest),1))))\n",
    "mydata.testLabels                = np.array([])\n",
    "mydata.createTrainValidateSplit()\n",
    "pool = None \n",
    "coef = None\n",
    "\n",
    "maj = Skylines.PRMWrapper(mydata, n_iter = 1000, tol = 1e-6, minC = 0, maxC = -1, minV = 0, maxV = -1,\n",
    "                            minClip = 0, maxClip = 0, estimator_type = 'Self-Normalized', verbose = True,\n",
    "                            parallel = pool, smartStart = coef)\n",
    "maj.calibrateHyperParams()\n",
    "maj.validate()\n",
    "Xtest1 = np.hstack((Xtest,np.ones((len(Xtest),1))))\n",
    "rec = Xtest1.dot(maj.labeler.coef_).argmax(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POEM_pol =  [bin_means[rec[i]-1] for i in range(n)]  \n",
    "# print POEM_pol\n",
    "POEM_risk = real_risk(POEM_pol, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)\n",
    "print np.mean(POEM_risk)\n",
    "print np.median(POEM_risk)\n",
    "plt.hist(POEM_risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w_dose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DM with random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_w_dose = np.column_stack((x, T))\n",
    "X_w_dose.shape\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "clf = clf.fit(X_w_dose[trainind,:],y=Y[trainind])\n",
    "data['x_full'] = X_w_dose[trainind,:]\n",
    "data['rf'] = clf\n",
    "print clf.score(X_w_dose,Y)\n",
    "\n",
    "def dm_wrapper(beta, args): \n",
    "    reg_lambda = 0.5\n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    x = data['x_full']\n",
    "    clf = data['rf']\n",
    "    tau = np.clip(np.dot( beta, x[:,:-1].T ), t_lo, t_hi).flatten()\n",
    "    counterfactual_X = np.column_stack((x[:,:-1], tau))\n",
    "    return (np.mean( clf.predict( counterfactual_X )) + reg_lambda * np.linalg.norm( beta, 2 )) \n",
    "\n",
    "dm_res = minimize(dm_wrapper, x0 = beta_d, bounds = bnds, options={'disp':True,'gtol':1e-1,'maxfun': 1000,'eps':np.ones(d)*1e-1}, args=data.items()) \n",
    "print dm_res.x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dm_T = np.dot(dm_res.x, x_test.T)\n",
    "\n",
    "plt.hist(real_risk(dm_T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test))\n",
    "np.mean(real_risk(dm_T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oracle_treatment(beta_cons, beta_x, beta_x_T, beta_x_quad_T, x): \n",
    "    res = minimize(real_risk_scalar, np.mean(T), args =(beta_cons, beta_x, beta_x_T, beta_x_quad_T,x))\n",
    "    return res.x \n",
    "n_test = x_test.shape[0]\n",
    "oracle_T = [ get_oracle_treatment(beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test[i,:]) for i in range(n_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.dot(disc_res.x, x_test.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x']\n",
    "data['tau'] = np.dot(disc_res.x, x_test.T)\n",
    "disc_pol_binned = np.digitize(np.dot(disc_res.x, x_test.T), bins).flatten()\n",
    "disc_pol = np.asarray( [bin_means[disc_pol_binned[i] - 1] for i in range(x_test.shape[0])] )\n",
    "plt.hist(disc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cons_pol_wrapper(beta, args): \n",
    "    data = dict(args)\n",
    "    t_lo = data['t_lo']; t_hi = data['t_hi']\n",
    "    tau = beta*np.ones(data['n'])\n",
    "    data['tau'] = tau\n",
    "    return data['sgn']*(off_policy_evaluation(**data))\n",
    "beta_t = np.random.uniform()*4\n",
    "bnds_t = tuple([(-15,15)])\n",
    "cons_res = minimize(cons_pol_wrapper, x0 = beta_t, bounds = bnds_t, options={'disp':True,'gtol':1e-1,'maxfun': 1000}, args=data.items())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure()\n",
    "\n",
    "# Use the same simulated loss function to estimate differences\n",
    "TRUE_ORACLE = real_risk(oracle_T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "oracle_loss = real_risk(np.dot(best_in_class.x, x_test.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "zero_pol = real_risk(np.zeros(n_test), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "mu_pol = real_risk(np.ones(n_test)*np.mean(T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "DM = real_risk(np.dot(dm_res.x, x_test.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "DR_oracle = real_risk(np.dot(DR_oracle_coefs[1][3],x_test.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "off_pol = real_risk(np.dot(oracle_coefs[1][1],x_test.T), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "disc_pol_norms = real_risk(disc_pol, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "best_constant = real_risk(cons_res.x*np.ones(n_test), beta_cons, beta_x, beta_x_T, beta_x_quad_T, x_test)\n",
    "\n",
    "\n",
    "original = real_risk(T, beta_cons, beta_x, beta_x_T, beta_x_quad_T, x)\n",
    "\n",
    "norms = [TRUE_ORACLE, off_pol,DR_oracle,DM, disc_pol_norms,best_constant, mu_pol, original ]\n",
    "\n",
    "plt.figure(figsize=(4.5,3))\n",
    "plt.ylabel('Thresholded absolute policy loss')\n",
    "flierprops = dict(linestyle='-',color='black')\n",
    "\n",
    "# prefer to turn off mean line\n",
    "medianprops = dict(linestyle='', linewidth=0, color='firebrick')\n",
    "f=plt.figure(figsize=(4.5,2.5))\n",
    "bp_dict = plt.boxplot(norms,sym='', showmeans = True, meanline=True,medianprops=medianprops)\n",
    "plt.ylabel('Outcome (loss)')\n",
    "for whisker in bp_dict['whiskers']:\n",
    "    whisker.set(color='#000000',linestyle='solid')\n",
    "for box in bp_dict['boxes']:\n",
    "    # change outline color\n",
    "    box.set( color='#000000')\n",
    "for ind,line in enumerate(bp_dict['means']):\n",
    "    # get position data for means line\n",
    "    xx, y = line.get_xydata()[1] # top of means line\n",
    "    if ind == 7: \n",
    "        plt.text(xx-1, 30, '%.1f' % y, horizontalalignment='center',size=13) # draw above, centered\n",
    "    # overlay median value\n",
    "    else:\n",
    "        plt.text(xx+0.2, -35, '%.1f' % y, horizontalalignment='center',size=13) # draw above, centered\n",
    "# top = 60\n",
    "# bottom = 0\n",
    "# plt.ylim(bottom, top)\n",
    "# plt.title('Boxplot of distances between policy-recommended doses and therapeutic doses',y = 1.2)\n",
    "# plt.axhline(y=np.mean(mean_dose_pol_norm), xmin=0,xmax=1,color='g', ls='dashdot',linewidth=0.7)\n",
    "# plt.axhline(y=0, xmin=0,xmax=1,color='g',alpha=0.3, ls='dashed',linewidth=0.7)\n",
    "plt.xticks([1, 2, 3,4,5,6,7,8], ['best o.o.c.', 'CPO', 'DR CPO', 'DM','Disc. CPO','CPE, cons.', 'Mean','Original'])\n",
    "plt.xticks(rotation=25)\n",
    "plt.ylim((-40,40))\n",
    "ax = plt.subplot(111)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "f.savefig(\"boxplot_risks.pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "f= plt.figure()\n",
    "plt.violinplot(norms)\n",
    "plt.xticks([1, 2, 3,4,5,6,7,8], ['Best o.o.c.','OPE', 'DR OPE','DM','Disc. OPE','OPE, cons.','Mean','Original'],size = 13)\n",
    "plt.xticks(rotation=25)\n",
    "plt.ylim((-90,70))\n",
    "for ind,line in enumerate(bp_dict['means']):\n",
    "    # get position data for means line\n",
    "    xx, y = line.get_xydata()[1] # top of means line\n",
    "    if ind == 6: \n",
    "        plt.text(xx+0.05, y+3.5, '%.1f' % y, horizontalalignment='center',size=13) # draw above, centered\n",
    "    # overlay median value\n",
    "    else:\n",
    "        plt.text(xx+0.24, y+3.5, '%.1f' % y, horizontalalignment='center',size=13) # draw above, centered\n",
    "\n",
    "        \n",
    "f.savefig(\"violin_plot_risks.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from statsmodels.nonparametric.kernel_density import EstimatorSettings\n",
    "\n",
    "\n",
    "settings = EstimatorSettings(efficient = True, randomize=True)\n",
    "list_data = [ x[trainind,i] for i in range(d) ] + [T[trainind]]\n",
    "kernel_regr = KernelReg(endog = [Y[trainind].reshape([len(trainind),1])], exog=list_data, var_type=['c']*len(list_data),bw='aic',defaults = settings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_regr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
